% This is "sig-alternate.tex" V2.1 April 2013
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate-05-2015}
\usepackage{xcolor}
\usepackage{xspace}
\hyphenpenalty=750
\newcommand\todo[1]{\textcolor{red}{** #1}}
\newcommand{\pooda}{\textsc{POOD}\xspace}

\newcommand{\pood}{\textsc{Porting OpenACC To OpenMP Directives}\xspace}

\newcommand{\squish}{
     { \setlength{\itemsep}{0pt}      \setlength{\parsep}{3pt}
       \setlength{\topsep}{3pt}       \setlength{\partopsep}{0pt}
       \setlength{\leftmargin}{1.5em} \setlength{\labelwidth}{1em}
       \setlength{\labelsep}{0.5em} } }

\begin{document}


% Copyright
\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\doi{}

% ISBN
\isbn{}

%Conference
\conferenceinfo{XSEDE '16}{July 17--21, 2016, Miami, FL, USA}

\acmPrice{\$15.00}

%
% --- Author Metadata here ---
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{From OpenACC to OpenMP 4: \\ Toward Automatic Translation}

\def\sharedaffiliation{%
\end{tabular}
\begin{tabular}{c}}

\numberofauthors{4}
\author{
\alignauthor
Nawrin Sultana \\
\email{nzs0034@auburn.edu}
\alignauthor
Alexander Calvert \\
\email{afc0005@auburn.edu}
\alignauthor
Jeffrey L.\@ Overbey \\
\email{joverbey@auburn.edu}
\sharedaffiliation
       \affaddr{Department of Computer Science and Software Engineering}\\
       \affaddr{Auburn University}\\
       \affaddr{Auburn University, AL, USA}
\and  % use '\and' if you need 'another row' of author names
\alignauthor
\alignauthor
Galen Arnold \\
\email{gwarnold@illinois.edu}
\alignauthor
\sharedaffiliation
       \affaddr{National Center for Supercomputing Applications}\\
       \affaddr{University of Illinois at Urbana-Champaign}\\
       \affaddr{Urbana, IL, USA}
}

\date{}

\maketitle
\begin{abstract}

\end{abstract}


%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>  
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}


%
% End generated code
%

%
%  Use this command to print the description
%
\printccsdesc

% We no longer use \terms command
%\terms{Theory}

\keywords{ACM proceedings; \LaTeX; text tagging}

\section{Introduction}

While CUDA and OpenCL are the canonical APIs for programming accelerator
devices, scientific programmers are increasingly turning to directive-based
APIs, which are more concise and declarative.  Since it first appeared in 2011,
OpenACC has been the standard directive-based API for scientific programmers
targeting NVIDIA GPUs.  OpenMP~4.0 added directives for accelerators, inspired
by OpenACC\@.  Both OpenACC and OpenMP are supported in both C/C++ and Fortran.

OpenACC is supported by PGI's compilers.  Intel's compilers support OpenMP~4.
Cray recently announced that their support for OpenACC will be frozen at
version~2.0, and future development will focus on OpenMP~4 device directives.
Recent versions of GCC support OpenACC; recent versions of Clang support
OpenMP~4.

So, the two APIs are rivals, at least for the time being.

Since nearly every compiler supports OpenMP for multicore parallelism, it is
reasonable to expect that the device directives added in OpenMP~4 will
eventually supplant OpenACC\@.  However, that certainly has not happened
yet---and given the strong backing for OpenACC from PGI/NVIDIA, there may
continue to be some rivalry between the two specifications.

In the XSEDE ecosystem, the PGI compilers are installed on Comet (at SDSC),
allowing OpenACC to be used for programming its NVIDIA GPU nodes.  At TACC,
portions of Stampede are being upgraded to the second-generation Intel Xeon Phi
(codenamed ``Knight's Landing''), which can be programmed using OpenMP~4.

Since OpenACC inspired the device directives added in OpenMP~4.0, the two APIs
are quite similar.  In many cases, there is a 1--1 mapping between directives.
However, there are some important differences as well.

In this paper, we investigate the feasibility of a tool to semi-automatically
convert OpenACC code to use OpenMP~4 device directives.  We have prototyped a
tool that automates a large part of the conversion process; we have also
identified parts of the conversion process that must be (or should be) left to
the programmer.

We focused on converting OpenACC to OpenMP~4 \emph{that targets the same
accelerator device}---an NVIDIA GPU.  We used PGI~C/C++~16.1 as our OpenACC
compiler, and we used Clang~3.8 as our OpenMP compiler.

We chose this setup because it allowed us to investigate changes in semantics
and performance that result from \emph{changing the compiler and API}, rather
than those that result from a change in target hardware.  Admittedly, for XSEDE
users, a more realistic scenario would be converting OpenACC on Comet (which
targets NVIDIA GPUs) to OpenMP on Stampede (which targets Xeon Phi's).
However, changing the target hardware in addition to the compiler and API
introduces additional complexity.  Keeping the target hardware fixed allowed us
to focus on the differences between OpenACC and OpenMP, rather than differences
in the accelerator devices themselves.

We used the EPCC Level~1 benchmarks as our test cases, along with a few
miscellaneous examples, and we focused on the subset of OpenACC used by these
programs.  Our results show that \emph{some} aspects of OpenACC-to-OpenMP
conversion can be automated, but some manual intervention is essential.
Also, both APIs rely heavily on the compiler, potentially making a change in
compilers one of the biggest obstacles in converting code from OpenACC to
OpenMP~4.

The remainder of this paper is organized as follows.
Section~\ref{sec:bg} gives a brief history and overview of OpenACC and OpenMP\@.
Section~\ref{sec:translation} discusses translation from OpenACC to OpenMP,
focusing on complications that prohibit completely automatic translation.
Section~\ref{sec:algorithm} describes our tool and the algorithm it uses to
convert OpenACC to OpenMP device directives.
Section~\ref{sec:eval} presents empirical data describing the conversion.
Section~\ref{sec:future} describes limitations of the current tool and
directions for future work.

\section{Background}
\label{sec:bg}

\subsection{History of OpenACC and OpenMP}

OpenACC has a short history. It was originally developed by PGI, Cray, and
NVIDIA, with version 1.0 being released in 2011. Version 2.0 was released in
2013, adding support for procedure calls and nested parallelization (enabled by
new CUDA capabilities), among other features. Version 2.5 was a more minor
revision, made in 2015.

%In the 1980's, SMP vendors typically defined directives for specifying how their SMP's divided up work on their individual processors. In the late 90's, 
OpenMP dates back to the late 1990s, when the OpenMP Architecture Review Board
was formed to develop a specification for directive-based parallelism that
could be used commonly across multiple vendors' multiprocessors. The first
OpenMP specification was released for Fortran in 1997 and for C/C++ in 1998.
The OpenMP specification has undergone several revisions since that time.
Notably, %task parallelism was added in version~3.0, and 
version~4.0 added support for accelerators in 2013.  The so-called
\emph{device directives} or \emph{target directives} added to support this
were based on OpenACC.  However, as we will see, the two APIs are not
identical.

\subsection{CUDA Execution Model}

OpenACC and OpenMP device directives are both designed to offload computations
to accelerator devices---massively parallel devices like GPUs and Xeon Phis,
designed for data parallel computations.  Each API has its own execution model,
which is intentionally abstract to avoid coupling it to the specifics of any
one device.

For the purposes of this paper, we will only focus on one type of accelerator
device: NVIDIA GPUs.  To understand the behavior of OpenACC and OpenMP
constructs on these devices, it is helpful to understand their execution
model---the CUDA execution model.

CUDA-capable devices (i.e., NVIDIA GPUs) execute \emph{kernels}, functions that
execute on the device by running many GPU threads concurrently. Only one kernel
executes at a time. All of the threads in a kernel are collectively called a
\emph{grid}. A grid is comprised of one or more \emph{thread blocks}, and each
thread block is comprised of one or more \emph{threads}.

In hardware, each thread block is assigned to one Streaming Multiprocessor
(SM). Each SM consists of several Streaming Processors (SPs); each thread of a
thread block is assigned to one of the SPs within the SM.  Thread blocks are
divided into 32-thread groups called \emph{warps}.  Instructions are fetched
and executed on a per-warp basis, not on a per-thread basis, so the 32 threads
within each warp execute instructions in single instruction, multiple data
(SIMD) fashion.  The SM context switches from one warp to another at every
instruction issue (``zero-overhead thread scheduling''), so while threads
within a warp execute in lockstep, threads in \emph{different} warps may not.
Threads within a block can coordinate using barrier synchronization, whereas
threads in different blocks cannot~\cite{kirk2012programming}.

\subsection{OpenMP~4 Directives}

\emph{Parallel.}
OpenMP's fundamental construct is the parallel construct. When a parallel
construct is encountered by a thread, that thread creates a \emph{team} of new
threads, becoming that team's master. The threads are assigned numbers, with
the master given number 0. The threads each execute a copy of the parallel
region's code. %None of the child threads may continue execution beyond the
%parallel region, and if any thread terminates within the region, the work done
%up to that point is undefined. Nested parallelism is legal in OpenMP, so
%parallel constructs may contain nested parallel constructs. 

\emph{For.}
Inside a parallel region, a \emph{for} construct may be specified. A for region
is associated with a for loop, and upon encountering the region, a different
thread from the current team is assigned to each iteration of the for loop. A
parallel for region is also defined as a shorthand for a parallel region
containing nothing but a single for region. OpenMP 4.0 adds a similar
construct, simd, which divides work of the associated loop up among SIMD lanes
of the current thread. 

\emph{Target Data.}
A target data region may be specified as well. A target data region creates a
data environment for execution. It may specify a device clause, which specifies
a device to execute the environment on, and a map clause, which specifies a
mapping of data from the host to the device. 

\emph{Target Teams.}
A target teams construct is equivalent to a target construct containing a teams
construct. The target teams construct creates a data environment and a
\emph{league} of thread teams to execute it. 

\emph{Distribute.}
A distribute construct creates a league of teams and specifies that contained
loops will be executed by those teams. 

\emph{Distribute Parallel For.}
A distribute parallel for construct combines distribute, parallel, and for
construct to specify that a loop can be executed in parallel by multiple
threads in multiple teams. 

\subsection{OpenACC Directives}

OpenACC has fewer constructs. This is in part because OpenACC is specifically
for use with massively parallel accelerator hardware, where OpenMP supports
accelerators as well as multicore CPUs. %As a result, task parallelism is not
%present in OpenACC.
However, many of OpenACC's construct are analogous to OpenMP ones. 

\emph{Parallel.}
OpenACC also has a parallel construct. A parallel construct, when encountered,
creates \emph{gangs of workers} to execute the region on the accelerator.

\emph{Loop.}
A loop construct is also available, specifying the type of parallelism for the
associated loop. There is also the parallel loop abbreviation, a shorthand for
a loop construct immediately inside a parallel construct. 

\emph{Kernels.}
The kernels construct specifies a region that is to be divided into a sequence
of kernels for accelerator execution. The loop and shorthand kernels loop are
available for kernels directives as they are for parallel directives. 

\emph{Data.}
A data directive is also defined. It specifies for a region what data should be
copied into the accelerator upon entry to the region and copied out on exit and
defines what data is to be allocated on the device for the duration of the
region. %Alternatively, the specification provides enter data and exit data
%directives that specify data allocation in the same way, but without defining a
%region -  enter data device allocations simply exist until exit data directives
%deallocate them.

\section{Translation Considerations}
\label{sec:translation}

%The major compilers supporting OpenACC are Portland Group (PGI), Cray, and CAPS. On the other hand, Intel has decided not to support OpenACC until accelerators are fully integrated into OpenMP. We describe an automated approach to assist programmers in converting OpenACC to OpenMP, so that same syntax could be used for both Cray and Intel compiler while maintaining the performance. We describe a refactoring (POOD) for this purpose.

Our goal was to create a tool to convert OpenACC code to equivalent OpenMP
code, automating as much of the process as possible.  For the purposes of
prototyping, we focused on translating C code (as opposed to C++ or Fortran).

\subsection{A Subset of OpenACC}
For the purposes of prototyping, we designed our tool to accept the subset of
OpenACC described by the grammar in Figure~\ref{fig:grammar}.  This includes
all of the directives and clauses used by the example programs in our test
suite (notably, the EPCC Level~1 benchmarks).  While it omits some important
features---asynchronous execution, unstructured data lifetimes, etc.---it does
correspond to a useful, commonly-used subset of OpenACC.

\begin{figure}
\begin{tabular}{rcl}
\textit{acc-directive} &$\rightarrow$& \texttt{\#pragma acc data\ }\textit{data-clauses} \\
                       &     $|$     & \texttt{\#pragma acc parallel}\texttt{\ }\textit{par-clauses} \\
                       &     $|$     & \texttt{\#pragma acc loop\ }\textit{type}\texttt{\ }\textit{loop-clauses} \\
\textit{type}          &$\rightarrow$& \texttt{gang vector} \\
                       &     $|$     & \texttt{gang} \\
                       &     $|$     & \texttt{vector} \\
                       &     $|$     & \texttt{seq} \\
%                       &     $|$     & $\epsilon$ \\
\textit{data-clause}   &$\rightarrow$& \texttt{copyin(}\textit{vars}\texttt{)} \\
                       &     $|$     & \texttt{copyout(}\textit{vars}\texttt{)} \\
                       &     $|$     & \texttt{copy(}\textit{vars}\texttt{)} \\
                       &     $|$     & \texttt{create(}\textit{vars}\texttt{)} \\
\textit{par-clause}&$\rightarrow$&\texttt{num\_gangs(}\textit{n}\texttt{)} \\
                       &     $|$     & \texttt{vector\_length(}\textit{n}\texttt{)} \\
                       &     $|$     & \texttt{private(}\textit{vars}\texttt{)} \\
                       &     $|$     & \texttt{reduction(}\textit{reductions}\texttt{)} \\
\textit{loop-clause}   &$\rightarrow$& \texttt{independent} \\
                       &     $|$     & \texttt{private(}\textit{vars}\texttt{)} \\
\end{tabular}
\caption{OpenACC subset grammar accepted by the translation tool.  Syntax is
omitted for \textit{vars}, \textit{reductions}, and natural numbers \textit{n}.
The nonterminals \textit{data-clauses} and \textit{loop-clauses} correspond to
sequences of zero or more phrases derived from \textit{data-clause} and
\textit{loop-clause}, respectively.  In addition to the directives shown,
\texttt{\#pragma acc parallel loop} is supported, and treated as syntactic
sugar for a \texttt{loop} directive nested directly under a \texttt{parallel}
directive.}
\label{fig:grammar}
\end{figure}

\subsection{Considerations in Translation}
In theory, the goals of the present paper are modest: We are interested in
translating a subset of OpenACC (Figure~\ref{fig:grammar}) to OpenMP~4,
targeting the same accelerator devices (NVIDIA GPUs).  In theory, the two APIs
are very closely aligned, and the translation should be straightforward.

In practice, the situation is not so simple.  The OpenACC and OpenMP
specifications define an \emph{abstract} execution model.  They do not dictate
how that model is mapped to specific hardware devices.  This is left to the
compiler.  Our OpenACC programs were compiled using PGCC~16.1, the standard
commercial compiler for OpenACC\@.  We compiled OpenMP~4 using a custom build
of Clang~3.8~\cite{clang} (since PGCC does not yet support
OpenMP~4).\footnote{Cray's compilers support both, but there are currently no
Cray resources in the XSEDE ecosystem.}  As we will show, directives that
appear similar may be translated quite differently by the two compilers.

\subsubsection{Work Distribution for Single Loops}

\begin{figure}
\begin{center}
\begin{minipage}{4in}
{\footnotesize
\begin{verbatim}
#pragma acc data copyin(A[0:N], B[0:N]), \
                 copyout(C[0:N])
#pragma omp target data map(to:A[0:N], B[0:N]), \
                        map(from:C[0:N])
{
  #pragma acc parallel loop
  #pragma omp target teams distribute parallel for
  for (int i = 0; i < N; ++i)
  {
    C[i] = A[i] + B[i];
  }
}
\end{verbatim}
}
\end{minipage}
\end{center}
\caption{Vector addition kernel.}
\label{fig:vecadd}
\end{figure}

Perhaps the simplest example of directive-based accelerator code is the vector
addition kernel shown in Figure~\ref{fig:vecadd}.  It adds two vectors $A$ and
$B$ on an accelerator device, storing the sum in $C$.  While the directives
direct the compiler to parallelize the loop on the accelerator, the compiler
can decide (in this case) how to divide the loop iterations among threads
and thread blocks on the CUDA device.  When $N = 10000$:
\begin{itemize}\squish
\item PGCC generates a grid of 79 thread blocks with 128 threads per block;
kernel execution time is about $3~\mu s$ on a Tesla~K40.
\item Clang generates a grid of 15 thread blocks with 1024 threads per block;
kernel execution time is about $30~\mu s$ on a Tesla~K40.\footnote{Adding
\texttt{num\_teams(79) thread\_limit(128)} to the OpenMP code (to match the
grid and block dimensions of the OpenACC code) actually increases the kernel
execution time to about $80~\mu s$.
%Adding \texttt{num\_gangs(15) vector\_length(1024)} to the OpenACC code does
%not change the performance significantly.
}
\end{itemize}

It is also worth noting that PGCC generates code where all threads are active
in all thread blocks except the last (presumably to minimize control
divergence).  Clang generates code that assigns an approximately equal number
of threads to each thread block, so inactive threads may be present in
several blocks.

\begin{figure*}
\begin{minipage}{.33\textwidth}
\emph{OpenACC (Original):}
{\footnotesize
\begin{verbatim}
#pragma acc parallel loop
for (int i = 0; i < 10; i++) {
  for (int j = 0; j < 10; j++) {
    ...
  }
}

\end{verbatim}
}
\centerline{(a)}
\end{minipage}
\begin{minipage}{.33\textwidth}
\emph{OpenACC (Equivalent):}
{\footnotesize
\begin{verbatim}
#pragma acc parallel loop gang
for (int i = 0; i < 10; i++) {
  #pragma acc parallel loop vector
  for (int j = 0; j < 10; j++) {
    ...
  }
}
\end{verbatim}
}
\centerline{(b)}
\end{minipage}
\begin{minipage}{.33\textwidth}
\emph{OpenMP~4:}
{\footnotesize
\begin{verbatim}
#pragma omp target teams distribute
for (int i = 0; i < 10; i++) {
  #pragma omp parallel for
  for (int j = 0; j < 10; j++) {
    ...
  }
}
\end{verbatim}
}
\centerline{(c)}
\end{minipage}
\caption{Doubly nested loops.}
\label{fig:nest}
\end{figure*}

\subsubsection{Work Distribution for Loop Nests}
\label{sec:nests}

More important is the handling of loop nests, such as the simple
doubly nested loops in Figure~\ref{fig:nest}(a).

In OpenACC, the compiler has some freedom to decide how to parallelize the loop
nest.  PGCC will typically run the outermost loop as a gang loop and the
innermost loop as a vector loop.  Given the OpenACC code in
Figure~\ref{fig:nest}(a), the compiler does exactly this: the iterations of the
$i$-loop are divided among the thread blocks, and the iterations of the
$j$-loop are divided among the threads within each block.  The OpenACC code in
Figure~\ref{fig:nest}(b) is functionally equivalent but makes this distribution
explicit.

In OpenMP, work distribution is not so automatic.  Consider these ``obvious''
but incorrect translations: The \texttt{\#pragma acc parallel loop} directive
could be replaced by:
\begin{itemize}\squish
\item \texttt{\#pragma omp target teams}.  In this case, the entire loop
nest would be run redundantly by all thread blocks.
\item \texttt{\#pragma omp target teams distribute}.  In this case, the
iterations of the $i$-loop would be divided among thread blocks, but only
one thread would be active within each block.
\item \texttt{\#pragma omp target teams distribute parallel for}.  Here,
the iterations of the $i$-loop are distributed among threads and thread blocks.
\end{itemize}
In all three cases, \emph{the inner $j$-loop runs sequentially}, because the
parallelization directive is applied to only one loop.  OpenMP requires the
programmer to explicitly specify how to run each loop in the nest.

The translation most similar to the OpenACC code is shown in
Figure~\ref{fig:nest}(c).  The iterations of the $i$-loop are divided among
thread blocks using the \texttt{distribute} directive, and the iterations of
the $j$-loop are divided among threads in the block using the \texttt{parallel
for} directive.
%
OpenACC's \texttt{gang} clause is roughly equivalent to OpenMP's
\texttt{distribute}---effectively, it distributes work among thread blocks.
Likewise, OpenACC's \texttt{vector} clause is roughly equivalent to OpenMP's
\texttt{parallel for} inside a \texttt{teams} region: it distributes work among
threads within a block.

%\subsubsection{Intrinsics Differ}

%\subsubsection{Reductions Differ}

\subsubsection{Data Transfer May Differ}

In both OpenACC and OpenMP, if \texttt{data}/\texttt{target data} directives
are omitted, the compiler will automatically determine what data needs to be
transferred between the host and the accelerator.  This is another case where
compilers may differ.  In the vector addition example
(Figure~\ref{fig:vecadd}), consider what happens when $A$, $B$, and $C$ are
\emph{pointers} allocated just before the kernel (in the same function) using
\textit{malloc}.  With PGCC, the \texttt{\#pragma acc data} directive can be
omitted; it can determine statically that 10,000 elements of $A$, $B$, and
$C$ are accessed and thus need to be copied.  With Clang, if the
\texttt{\#pragma omp target data} directive is omitted, the kernel crashes at
runtime (\emph{device illegal address}).

\section{Translation Algorithm and \\ Tool Implementation}
\label{sec:algorithm}

In the previous section, we showed that OpenACC and OpenMP~4 device directives
handle work distribution differently, and that differences in compilers can
have significant effects on the performance (and correctness) of code.  This
means that porting from OpenACC to OpenMP~4 will almost certainly involve some
manual effort.  Nevertheless, much of the translation is straightforward and
tedious, which makes it worth automating.

Therefore, we designed our translation tool---and its underlying
algorithm---with the following objectives:
\begin{itemize}
\item \emph{The translation should be straightfoward and consistent.}
Each OpenACC directive should translate to the same OpenMP directive(s)
every time, and the translation should mirror the structure of the OpenACC
code, to the extent possible.
\item \emph{Automated translation and manual editing should be complementary.}
The tool should not try to automate too much.  It should not perform loop
nest optimizations; it should not infer implicit data transfers; it should
not make any invasive code changes.  The tool should automate the tedious,
most obvious parts of the translation, in a completely predictable way.
More sophisticated changes---like performance tuning---should be under the
control of the programmer.
\end{itemize}

The translation algorithm is as follows.  The input is a C source file
containing OpenACC directives.  The output is the same file with OpenMP
directives substituted.
\begin{enumerate}
\item If any of the following occur, warn the user; these structures will
not be automatically converted:
\begin{itemize}
\item The header file \texttt{openacc.h} is included.
\item The macro \texttt{\_OPENACC} is used.
\item Functions in the OpenACC runtime library are called or referenced.
\item The \texttt{kernels} directive is used.
\end{itemize}
\item Prepare parallel loop nests using the algorithm described in
\S\ref{sec:loops} below.
\item Apply the translation rules described below in \S\ref{sec:directives} to
convert OpenACC directives to OpenMP directives.
\end{enumerate}

The \texttt{\#pragma acc kernels} directive is effectively a request for
automatic parallelization; the compiler must decide what loops to parallelize
and how.  There is no direct equivalent in OpenMP\@.  Thus, our algorithm does
not support the kernels directive.  Instead, the programmer should modify the
code to explicitly parallelize loops using the \texttt{parallel} directive.

\subsection{Loop Nest Preparation}
\label{sec:loops}

As noted in \S\ref{sec:nests} above, OpenACC-to-OpenMP translation is
complicated by the fact that work distribution is explicit in OpenMP, while it
may be left to the compiler in OpenACC\@.  We noted that the code in
Figure~\ref{fig:nest}(a) is equivalent to that in Figure~\ref{fig:nest}(b),
expect the latter makes the work distribution explicit.

Before our translation tool attempts to translate any OpenACC directives,
it ``prepares'' each parallel loop nest as follows.
\begin{itemize}\squish
\item If the loop nest contains explicit gang and vector clauses, there must be
exactly one gang loop and exactly one vector loop in the nest.  The same loop
may be both the gang and vector loop.
\item If the loop nest does not contain an explicit gang loop, the outermost
non-sequential loop is chosen as the gang loop.
\item If the loop nest does not contain an explicit vector loop, the innermost
non-sequential loop is chosen as the vector loop.
\item All loops in the loop nest other than the gang and vector loop are run
sequentially.
\item If a loop will be run sequentially, and it contains a \texttt{private}
clause, all \texttt{private} variables must not be live at the top of the loop.
\item If the loop nest does not have the form above (e.g., if it contains two
gang loops, or if a vector loop appears outside a gang loop), the translation
to OpenMP cannot proceed.
\end{itemize}

The last rule is necessary because OpenACC is surprisingly lax about where the
nesting of gang and vector loops.  For example, a \texttt{gang} loop may be
nested inside a \texttt{vector} loop; there is no direct equivalent of this in
OpenMP.  Moreover, PGCC may ignore the \texttt{gang} and \texttt{vector}
directives altogether, e.g., when a gang loop appears inside another gang loop,
or a vector loop appears outside another vector loop.

The next-to-last rule ensures that, if a loop is run sequentially, the
\texttt{private} clause can be removed without changing the behavior of the
loop.  Although the rule is stated in terms of a live variables
analysis~\cite{kennedy}, the concept is straightforward.  If a variable $a$
is declared as private, but each iteration assigns a value to $a$ before
reading its value, then the \texttt{private} clause can be eliminated if
the loop is run sequentially.  It cannot be removed if each iteration of the
loop depends on the value of $a$ prior to the loop (e.g., if the first
instruction in the loop was \texttt{a++}).

The second and third rules implement a heuristic typical of parallelizing
compilers (one that PGCC appears to use, in fact): the iterations of the
outermost parallelizable loop are partitioned among thread blocks, and the
iterations of the innermost parallelizable loop are partitioned among the CUDA
threads within that block.

To determine if a loop is parallelizable (for the second and third rules), our
tool uses a combination of syntactic checks and dependence
analysis~\cite{wolfe,kennedy}:
\begin{enumerate}\squish
\item If the loop contains a \texttt{seq} clause, it is not parallelizable.
\item If the loop contains a \texttt{gang} or \texttt{vector} clause, it is
parallelizable.
\item Otherwise, we perform a dependence analysis on the loop.  If the loop
does not carry a dependence, it is parallelizable~\cite{kennedy}; if it
does, we determine it to be non-parallelizable.
\end{enumerate}

\subsection{Directive Translation}
\label{sec:directives}

\newcommand{\translate}[1]{\ensuremath{\mathcal{T}[\![ #1 ]\!]}}
\begin{figure*}
\begin{align*}
%
\translate{\texttt{\#pragma acc data\ }\textit{data-clauses}} &=
   \texttt{\#pragma omp target data } \translate{\textit{data-clauses}} \\
%
\translate{\texttt{copyin(}\textit{vars}\texttt{)}} &=
   \texttt{map(to:}\textit{vars}\texttt{)} \\
%
\translate{\texttt{copyout(}\textit{vars}\texttt{)}} &=
   \texttt{map(from:}\textit{vars}\texttt{)} \\
%
\translate{\texttt{copy(}\textit{vars}\texttt{)}} &=
   \texttt{map(tofrom:}\textit{vars}\texttt{)} \\
%
\translate{\texttt{create(}\textit{vars}\texttt{)}} &=
   \texttt{map(alloc:}\textit{vars}\texttt{)} \\
%
\translate{\texttt{\#pragma acc parallel\ }\textit{par-clauses}} &=
   \left.\parbox[t]{4in}{%
   \texttt{\#pragma omp target teams\ }\translate{\textit{par-clauses}}}\right. \\
%
\translate{\texttt{num\_gangs(}\textit{n}\texttt{)}} &=
   \texttt{num\_teams(}\textit{n}\texttt{)} \\
%
\translate{\texttt{vector\_length(}\textit{n}\texttt{)}} &=
   \texttt{thread\_limit(}\textit{n}\texttt{)} \\
%
\translate{\texttt{private(}\textit{vars}\texttt{)}} &=
   \texttt{private(}\textit{vars}\texttt{)} \\
%
\translate{\texttt{reduction(}\textit{vars}\texttt{)}} &=
   \texttt{reduction(}\textit{reductions}\texttt{)} \\
%
\translate{\texttt{\#pragma acc loop\ }\textit{type}\texttt{\ }\textit{loop-clauses}} &=
   \begin{cases}
   \texttt{\#pragma omp distribute parallel for\ }\translate{\textit{loop-clauses}} \\
        \hspace*{2em} \textup{ if $\textit{type} = \texttt{gang vector}$} \vspace*{.25em} \\
   \texttt{\#pragma omp distribute\ }\translate{\textit{loop-clauses}} \\
        \hspace*{2em} \textup{ if $\textit{type} = \texttt{gang}$} \vspace*{.25em} \\
   \texttt{\#pragma omp parallel for\ }\translate{\textit{loop-clauses}} \\
        \hspace*{2em} \textup{ if $\textit{type} = \texttt{vector}$} \vspace*{.25em} \\
   \epsilon
        \hspace*{1.65em} \textup{ if $\textit{type} = \texttt{seq}$} \\
   \end{cases} \\
%
\translate{\texttt{independent}} &=
   \epsilon \\
%
\end{align*}
\caption{Translation rules from OpenACC to OpenMP~4.  The function
$\mathcal{T}$ maps phrases derived from the OpenACC subset grammar to
strings corresponding to one or more OpenMP directives.  Sequences
(e.g., \textit{par-clauses}) are translated elementwise, that is,
$\translate{t_1\ t_2\ \dots\ t_n} =
\translate{t_1}\ \translate{t_2}\ \dots\ \translate{t_n}$.}
The symbol $\epsilon$ denotes the empty string.
\label{fig:rules}
\end{figure*}

The loop nest preparation algorithm in the previous section identifies a single
gang loop and a single vector loop in each parallel loop nest, and it guarantees
that the vector loop is nested under (or is the same as) the gang loop.  After
gang, vector, and sequential loops have been identified, it is possible to
proceed with translating OpenACC directives to OpenMP directives.

Our tool implements the translation rules in Figure~\ref{fig:rules}.  The
majority of the translation is straightforward: Each phrase in OpenACC is
mapped to a corresponding phrase in OpenMP\@.

It is important to note that, according to the translation rules in
Figure~\ref{fig:rules}, \emph{each data directive is translated as-is}.  No
attempt is made to infer implicit data transfers or array lengths.

Also, these rules assume that each loop in a parallel loop nest has been
\emph{explicitly} labeled with a \texttt{gang}, \texttt{vector}, or
\texttt{seq} clause (or both \texttt{gang} and \texttt{vector}).  Of course,
this is not necessary.  In fact, our tool runs the loop nest preparation
algorithm implicitly; it identifies a gang and vector loop in each parallel
loop nest, but it does not literally insert OpenACC \texttt{gang},
\texttt{vector}, and \texttt{seq} clauses into the source code (although
it retains them if they are already present in the code).

\subsection{Translation Tool}

We prototyped our OpenACC-to-OpenMP translation algorithm by extending the
Eclipse C/C++ Development Tools.  In addition to our translation algorithm, we
added support for various data flow analyses and dependence analysis, along
with a parser for OpenACC directives.  Our translation can be performed from
within the Eclipse user interface, but we also added a command line interface
(CLI), so one can run the translation from the command line, without installing
Eclipse. 

%Our tool does not handle all the OpenACC directives. We transformed the directives present in EPCC level 1 OpenACC benchmark except ``kernel" directive. When user invoke a refactoring on a C source file, refactoring engine performs the precondition checking. If preconditions are violated, it shows appropriate error message to user. Then analyses and source translations are executed.
%  
%\subsection{Porting OpenACC To OpenMP Directives (POOD)}
%The \pood refactoring converts OpenACC directives to OpenMP 4.0 directives.
%
%\textbf{Motivation:} While porting OpenACC to OpenMP it is necessary for directives to be compatible. Without automated tooling, a programmer has to manually transform each directives.
%
%\textbf{Precondition:} A programmer selects a C source file and invokes the \pooda transformation. The following precondition is checked:
%
%\begin{itemize}
%\item Source file contains OpenACC directives.
%\item OpenACC directives in source code supported by our transformation tool.
%\end{itemize}  
%
%\textbf{Mechanism:} The refactoring introduces OpenMP directives that correspond to OpenACC directives.
%
%POOD first determines all the preprocessor include statements and check whether there is any ``openacc.h". If openacc header presents POOD removes it.\todo{need to remove openacc functions}. From the precondition checking we have the OpenACC preprocessor pragma statements that we want to transform. For each directives Abstract Syntax Tree gives the exact position. POOD removes the existing pragma statment and inserts OpenMP directives on that position based on the directive clause. Table \ref{table:1} lists the OpenACC and corresponding OpenMP clauses that POOD transforms. For any OpenACC data directives with copy, copyin, or copyout clause POOD performs NameBinding analyses\todo{fix} for the variables to find out their declaration. If these variables are declared globally then POOD changes them to local. When all the transformations have completed, POOD writes the changes to file.
%
%
%\begin{table}[t]
%	\begin{minipage}{0.47\textwidth}
%%\small
%\caption{Equivqlent OpenACC and OpenMP clause}
%\centering
%\begin{tabular}{ | l@{\hspace{0.1cm}}|  @{\hspace{0.4cm}}r@{\hspace{0.1cm}}|} 
% \hline
% \hspace*{0.4cm} OpenACC Clause & OpenMP Clause\hspace*{0.6cm}\\
% \hspace*{0.5cm} (\#pragma acc)%\footnote{The ID is used to refer to each application in the rest of the paper.} 
%  & (\#pragma omp)\hspace*{0.6cm} \\
% \hline
% data & target data \\ 
% copy(A[0:n*n]) & map(tofrom:A[0:n*n]) \\
% copyin(A[0:n*n]) & map(to:A[0:n*n]) \\
% copyout(A[0:n*n]) & map(from:A[0:n*n]) \\
% create(A[0:n*n]) & map(to:A[0:n*n]) \\ 
% parallel & target/teams \\ 
% parallel loop private(t1,t2) & parallel for private(t1,t2) \\
% loop & for \\
% loop private(tmp) & for private(tmp) \\
% loop reduction(+:t1,t2) & for reduction(+:t1,t2) \\
% loop independent & for independent \\
% parallel loop & target/teams/distribute \\
% \hline
%\end{tabular}
%
%\label{table:1}
%\end{minipage}
%\end{table}
%%\normalsize
%
%
%\textbf{Example:}

\section{Evaluation}
\label{sec:eval}

%\begin{table}
%\begin{tabular}{ | l@{\hspace{0.1cm}}|  @{\hspace{0.4cm}}r@{\hspace{0.1cm}}|} 
%Test Code    & Number of Directives \\
%EPCC Level~1 & \\
%
%\end{tabular}
%\end{table}

To evaluate the correctness of our tool, we used it to translate the EPCC
Level~1 OpenACC benchmarks~\cite{epcc}, which are ports of the PolyBench and
PolyBench/GPU kernels~\cite{polybench}.  We also translated several
miscellaneous test programs, including naive and tiled matrix multiplication
kernels.  We compiled the original OpenACC code using PGCC~16.1 and the
resulting OpenMP code using Clang~3.8.  Performance measurements were taken on
a Tesla K40 (Comet and Stampede have Tesla K20 GPUs).

The EPCC Level~1 benchmarks contain a total of 70 OpenACC directives, three of
which are \emph{kernels} directives that our tool would not translate directly.
Of the remaining 67 directives, 13 were data directives, 23 were parallel loop
directives, and 31 were loop directives nested under other parallel directives.
There was only one OpenACC API call: a call to \texttt{acc\_init}, used to
initialize the GPU and the OpenACC runtime.

This suggests that a tool like ours could be quite valuable.  Only two changes
needed to be made by the programmer: removing \texttt{acc\_init} and converting
the three \emph{kernels} regions.  The remaining 67 directives could all be
converted automatically, without manual intervention.

\section{Limitations and Future Work}
\label{sec:future}

Our tool is a prototype, and our translation algorithm supports only a few
commonly used OpenACC directives.  Some areas for improvement are obvious.  The
algorithm needs to be expanded to include the entire OpenACC specification,
including asynchronous kernel launches, unstructured data lifetimes, and
OpenACC API calls.  Our tool works only on C, while OpenACC and OpenMP both
support C++ and Fortran.  While the EPCC benchmarks were useful for evaluating
our prototype, we need to test its effectiveness on larger code bases (i.e.,
application codes), ideally by working with other XSEDE collaborators.

While automating a tedious 1--1 translation is helpful, it is not the end of
the conversion process.  As noted earlier, a common XSEDE use case will likely
be converting OpenACC code to take advantage of the forthcoming
second-generation Intel Xeon Phis on Stampede.  Our translation algorithm (and
tool) could certainly be specialized for this process.

After the initial OpenMP code has been created, some restructuring will almost
certainly be necessary.  For example, we have performed some preliminary
experiments with Intel's compilers on first-generation Phis; in one case, we
needed to convert global variables to local variables to ensure that the OpenMP
\emph{target data} directive would guarantee a copy of the arrays to the Phi.
Cataloging (and perhaps automating) such transformations would be beneficial.

Perhaps more importantly, performance portability is not guaranteed.  A direct
translation of OpenACC code optimized for an NVIDIA GPU will not necessary
perform at its peak on a Xeon Phi.

Interestingly, we did not even obtain comparable performance targeting the
\emph{same} hardware.  In our test cases, the GPU kernels generated by Clang
were almost always several times slower than those generated by PGCC.  For
example, consider the naive matrix multiplication kernel in
Figure~\ref{fig:matmul}.  With $2048 \times 2048$ matrices, the OpenACC kernel
ran for about 250~ms; the OpenMP kernel ran for about 925~ms.  Since the
OpenACC and OpenMP codes were nearly identical, and they were running on the
same hardware, we did not expect such massive differences in performance.
Clearly, details of the compiler-generated code can have a substantial impact
on performance.  Again, documenting differences in compilers would be
helpful---what sort of performance change is typical, what can be done to
improve performance when changing from compiler X to compiler Y, etc.

\begin{figure}
\begin{verbatim}
#pragma omp target teams distribute
#pragma acc parallel loop gang
for (int j = 0; j < N; j++) {
    #pragma omp parallel for
    #pragma acc loop vector
    for (int i = 0; i < N; i++) {
        double t = 0.0;
        #pragma acc loop seq
        for (int k = 0; k < N; k++) {
            t += m[k][i] * n[j][k];
        }
        p[j][i] = t;
    }
}
\end{verbatim}
\caption{Naive matrix multiplication.}
\label{fig:matmul}
\end{figure}

%\section{Conclusion}


%ACKNOWLEDGMENTS are optional
%\section{Acknowledgments}

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns
%\appendix
\end{document}
